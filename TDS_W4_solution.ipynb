{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "inputs = document.querySelectorAll('input')\n",
        "\n",
        "textboxes = document.querySelectorAll(\"textarea\")\n",
        "\n",
        "buttons = document.querySelectorAll(\"button\")\n",
        "\n",
        "inputs.forEach(input => input.removeAttribute('disabled'));\n",
        "\n",
        "buttons.forEach(input => input.removeAttribute('disabled'));\n",
        "\n",
        "textboxes.forEach(input => input.removeAttribute('disabled'));"
      ],
      "metadata": {
        "id": "gTnx4KOuharn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important Note :\n",
        "This colab file to help student who stuck on some question.\n",
        "First try each question by yourself, if you are unable to do it you can take help from it."
      ],
      "metadata": {
        "id": "jJrVMhEtwn4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To solve this module, we need to have AI Proxy Tokens.\n",
        "1. Get your token at: https://aiproxy.sanand.workers.dev/\n",
        "\n",
        "2. Sign in with your iitm id and u'll get one.(Remember, we have limited usage, use them wisely)"
      ],
      "metadata": {
        "id": "OQdNrvPuwzAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "1. Open google sheet\n",
        "2. Use the IMPORTHTML function to fetch the table data from the URL. The formula should be : =IMPORTHTML(\"https://stats.espncricinfo.com/stats/engine/stats/index.html?class=2;page=19;template=results;type=batting\", \"table\", 3)\n",
        "3. it will create a table, find out 0 value column\n",
        "4. sum of all values in 0 column is number of ducks\n"
      ],
      "metadata": {
        "id": "zRxatQr8b_q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        "\n",
        "1. Open IMDb Advanced Search\n",
        "Go to https://www.imdb.com/search/title/\n",
        "Apply the filter for movies with ratings between 3 and 5\n",
        "2. Press F12 or Ctrl + Shift + I (Windows) / Cmd + Option + I (Mac) to open DevTools.\n",
        "3. Click on the Console tab.\n",
        "4. Run below code in console , you will get json data for answer\n",
        "5. note: it will throw error like expected value and actual value, modify those values manually based on expected values ( it also tells location where to change values [0]-means movie 1, [1]-means movie 2 and so on )"
      ],
      "metadata": {
        "id": "a9kWliQhd632"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "const movies = [];\n",
        "\n",
        "document.querySelectorAll('.ipc-metadata-list-summary-item').forEach((item, index) => {\n",
        "    if (index >= 25) return;\n",
        "\n",
        "    const titleElement = item.querySelector('.ipc-title__text');\n",
        "    const yearElement = item.querySelector('.sc-f30335b4-7.jhjEEd.dli-title-metadata-item');\n",
        "    const ratingElement = item.querySelector('.ipc-rating-star--rating');\n",
        "\n",
        "    if (titleElement && yearElement) {\n",
        "        const idMatch = item.querySelector('a[href*=\"/title/tt\"]')?.href.match(/tt\\d+/);\n",
        "        const id = idMatch ? idMatch[0] : null;\n",
        "        const title = titleElement.textContent.trim();\n",
        "\n",
        "        // DO NOT trim the year, and replace non-breaking space with visible space if needed\n",
        "        let year = yearElement.textContent;\n",
        "        year = year.replace(/\\u00A0/g, ' '); // Replace non-breaking space with regular space\n",
        "\n",
        "        const ratingText = ratingElement?.textContent.trim();\n",
        "        const rating = ratingText || null;\n",
        "\n",
        "        if (id && title && year && rating) {\n",
        "            const ratingFloat = parseFloat(rating);\n",
        "            if (!isNaN(ratingFloat) && ratingFloat >= 3 && ratingFloat <= 5) {\n",
        "                movies.push({ id, title, year, rating });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "});\n",
        "\n",
        "console.log(JSON.stringify(movies));\n"
      ],
      "metadata": {
        "id": "zGEbQIKad8Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "\n",
        "def fetch_filtered_imdb_titles(min_rating=3.0, max_rating=5.0):\n",
        "    # IMDb search URL without title_type filter\n",
        "    url = f\"https://www.imdb.com/search/title/?user_rating={min_rating},{max_rating}\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\"\n",
        "\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to fetch page:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    movies = []\n",
        "\n",
        "    # Select up to 25 movie items\n",
        "    movie_items = soup.select('.ipc-metadata-list-summary-item')[:25]\n",
        "\n",
        "    for item in movie_items:\n",
        "        title_element = item.select_one('.ipc-title__text')\n",
        "        year_element = item.select_one('.sc-f30335b4-7.jhjEEd.dli-title-metadata-item')\n",
        "        rating_element = item.select_one('.ipc-rating-star--rating')\n",
        "\n",
        "        if title_element and year_element:\n",
        "            # Extract ID\n",
        "            link_tag = item.select_one('a[href*=\"/title/tt\"]')\n",
        "            match = re.search(r'tt\\d+', link_tag['href']) if link_tag else None\n",
        "            imdb_id = match.group(0) if match else None\n",
        "\n",
        "            # Extract and clean fields\n",
        "            title = title_element.get_text(strip=True)\n",
        "            year = year_element.get_text().replace('\\xa0', ' ')  # Preserve NBSP\n",
        "            rating = rating_element.get_text(strip=True) if rating_element else None\n",
        "\n",
        "            try:\n",
        "                rating_float = float(rating)\n",
        "                if min_rating <= rating_float <= max_rating:\n",
        "                    movies.append({\n",
        "                        \"id\": imdb_id,\n",
        "                        \"title\": title,\n",
        "                        \"year\": year,\n",
        "                        \"rating\": rating\n",
        "                    })\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "\n",
        "    return json.dumps(movies, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    movies = fetch_filtered_imdb_titles(3.0, 5.0)\n",
        "    print(movies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4lhq_CaZgXlO",
        "outputId": "e2084247-659c-4e49-d44f-bd301efe81e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": \"tt32047217\",\n",
            "    \"title\": \"1. Delicious\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"4.4\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt35836959\",\n",
            "    \"title\": \"2. Medusa\",\n",
            "    \"year\": \"2025– \",\n",
            "    \"rating\": \"5.0\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt4419684\",\n",
            "    \"title\": \"3. In the Lost Lands\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"4.9\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt35257773\",\n",
            "    \"title\": \"4. With Love, Meghan\",\n",
            "    \"year\": \"2025– \",\n",
            "    \"rating\": \"3.1\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt11656220\",\n",
            "    \"title\": \"5. Midnight in the Switchgrass\",\n",
            "    \"year\": \"2021\",\n",
            "    \"rating\": \"4.6\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt30422937\",\n",
            "    \"title\": \"6. Nadaaniyan\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"3.0\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt22475008\",\n",
            "    \"title\": \"7. Watson\",\n",
            "    \"year\": \"2024– \",\n",
            "    \"rating\": \"5.0\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt30789086\",\n",
            "    \"title\": \"8. Control Freak\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"4.5\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt4978420\",\n",
            "    \"title\": \"9. Borderlands\",\n",
            "    \"year\": \"2024\",\n",
            "    \"rating\": \"4.7\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt26757462\",\n",
            "    \"title\": \"10. Hellboy: The Crooked Man\",\n",
            "    \"year\": \"2024\",\n",
            "    \"rating\": \"4.5\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt0052289\",\n",
            "    \"title\": \"11. The Thing That Couldn't Die\",\n",
            "    \"year\": \"1958\",\n",
            "    \"rating\": \"4.3\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt2322441\",\n",
            "    \"title\": \"12. Fifty Shades of Grey\",\n",
            "    \"year\": \"2015\",\n",
            "    \"rating\": \"4.2\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt10128846\",\n",
            "    \"title\": \"13. Megalopolis\",\n",
            "    \"year\": \"2024\",\n",
            "    \"rating\": \"4.7\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt4113114\",\n",
            "    \"title\": \"14. Lethal Seduction\",\n",
            "    \"year\": \"2015\",\n",
            "    \"rating\": \"5.0\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt30253036\",\n",
            "    \"title\": \"15. Kinda Pregnant\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"4.9\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt10886166\",\n",
            "    \"title\": \"16. 365 Days\",\n",
            "    \"year\": \"2020\",\n",
            "    \"rating\": \"3.3\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt1340094\",\n",
            "    \"title\": \"17. The Crow\",\n",
            "    \"year\": \"2024\",\n",
            "    \"rating\": \"4.7\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt28784380\",\n",
            "    \"title\": \"18. Paradis City\",\n",
            "    \"year\": \"2025– \",\n",
            "    \"rating\": \"4.7\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt1467304\",\n",
            "    \"title\": \"19. The Human Centipede (First Sequence)\",\n",
            "    \"year\": \"2009\",\n",
            "    \"rating\": \"4.4\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt5304688\",\n",
            "    \"title\": \"20. Bad Sister\",\n",
            "    \"year\": \"2015\",\n",
            "    \"rating\": \"4.7\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt32982109\",\n",
            "    \"title\": \"21. The Divorced Billionaire Heiress\",\n",
            "    \"year\": \"2024\",\n",
            "    \"rating\": \"4.7\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt9023964\",\n",
            "    \"title\": \"22. Silent Zone\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"4.8\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt3399484\",\n",
            "    \"title\": \"23. The Sand\",\n",
            "    \"year\": \"2015\",\n",
            "    \"rating\": \"3.8\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt30956852\",\n",
            "    \"title\": \"24. Popeye: The Slayer Man\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"3.8\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"tt33362807\",\n",
            "    \"title\": \"25. Popeye's Revenge\",\n",
            "    \"year\": \"2025\",\n",
            "    \"rating\": \"4.0\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.imdb.com/search/title/?user_rating=3.0,5.0\"\n",
        "headers = {\"Accept-Language\": \"en-US,en;q=0.5\"}\n",
        "res = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "movies = []\n",
        "for idx, div in enumerate(soup.select(\".ipc-metadata-list-summary-item\"), start=1):\n",
        "    header = div.find(\"h3\", class_=\"ipc-title_text\")\n",
        "    imdb_id = div.find(\"a\")[\"href\"].split(\"/\")[2]\n",
        "    title = header.find(\"a\").text.strip()\n",
        "    year = header.find(\"span\", class_=\"sc-f30335b4-7.jhjEEd.dli-title-metadata-item\").text.strip()\n",
        "    rating_tag = div.find(\"div\", class_=\"ipc-rating-star--rating\")\n",
        "    rating = rating_tag[\"data-value\"] if rating_tag else \"N/A\"\n",
        "\n",
        "    movies.append({\n",
        "        \"id\": imdb_id,\n",
        "        \"title\": f\"{idx}. {title}\",\n",
        "        \"year\": year,\n",
        "        \"rating\": rating\n",
        "    })\n",
        "\n",
        "print(movies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGdXrFK_HFkc",
        "outputId": "8049ef3e-3b2f-4b03-9005-ba514b7ee712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3\n",
        "\n",
        "1. open vs code and create new file name as main.py\n",
        "\n",
        "2. write down below code\n",
        "\n",
        "3. type \"python -m uvicorn main:app --reload\" in the powershell.\n",
        "\n",
        "4. After running succesfully, paste this in portal: http://127.0.0.1:8000\n"
      ],
      "metadata": {
        "id": "xS4DpUvIb_jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import markdown\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow CORS from any origin\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "def get_wikipedia_url(country: str) -> str:\n",
        "    \"\"\"\n",
        "    Given a country name, returns the Wikipedia URL for the country.\n",
        "    \"\"\"\n",
        "    return f\"https://en.wikipedia.org/wiki/{country}\"\n",
        "\n",
        "def extract_headings_from_html(html: str) -> list:\n",
        "    \"\"\"\n",
        "    Extract all headings (H1 to H6) from the given HTML and return a list.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    headings = []\n",
        "\n",
        "    # Loop through all the heading tags (H1 to H6)\n",
        "    for level in range(1, 7):\n",
        "        for tag in soup.find_all(f'h{level}'):\n",
        "            headings.append((level, tag.get_text(strip=True)))\n",
        "\n",
        "    return headings\n",
        "\n",
        "def generate_markdown_outline(headings: list) -> str:\n",
        "    \"\"\"\n",
        "    Converts the extracted headings into a markdown-formatted outline.\n",
        "    \"\"\"\n",
        "    markdown_outline = \"## Contents\\n\\n\"\n",
        "    for level, heading in headings:\n",
        "        markdown_outline += \"#\" * level + f\" {heading}\\n\\n\"\n",
        "    return markdown_outline\n",
        "\n",
        "@app.get(\"/api/outline\")\n",
        "async def get_country_outline(country: str):\n",
        "    \"\"\"\n",
        "    API endpoint that returns the markdown outline of the given country Wikipedia page.\n",
        "    \"\"\"\n",
        "    if not country:\n",
        "        raise HTTPException(status_code=400, detail=\"Country parameter is required\")\n",
        "\n",
        "    # Fetch Wikipedia page\n",
        "    url = get_wikipedia_url(country)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise HTTPException(status_code=404, detail=f\"Error fetching Wikipedia page: {e}\")\n",
        "\n",
        "    # Extract headings and generate markdown outline\n",
        "    headings = extract_headings_from_html(response.text)\n",
        "    if not headings:\n",
        "        raise HTTPException(status_code=404, detail=\"No headings found in the Wikipedia page\")\n",
        "\n",
        "    markdown_outline = generate_markdown_outline(headings)\n",
        "    return JSONResponse(content={\"outline\": markdown_outline})\n"
      ],
      "metadata": {
        "id": "-lHW3oYicBJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4\n",
        "\n",
        "\n",
        "1. run below code\n",
        "2. change city as per your question\n",
        "3. copy json file to portal"
      ],
      "metadata": {
        "id": "erD8HpOAdsOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "required_city = \"Auckland\"\n",
        "location_url = 'https://locator-service.api.bbci.co.uk/locations?' + urlencode({\n",
        "   'api_key': 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv',\n",
        "   's': required_city,\n",
        "   'stack': 'aws',\n",
        "   'locale': 'en',\n",
        "   'filter': 'international',\n",
        "   'place-types': 'settlement,airport,district',\n",
        "   'order': 'importance',\n",
        "   'a': 'true',\n",
        "   'format': 'json'\n",
        "})\n",
        "\n",
        "# Fetch location data\n",
        "result = requests.get(location_url).json()\n",
        "weather_url = 'https://www.bbc.com/weather/' + result['response']['results']['results'][0]['id']\n",
        "\n",
        "# Fetch weather data\n",
        "response = requests.get(weather_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "daily_summary = soup.find('div', attrs={'class': 'wr-day-summary'})\n",
        "daily_summary_list = re.findall('[a-zA-Z][^A-Z]*', daily_summary.text)\n",
        "\n",
        "\n",
        "# Generate date list\n",
        "datelist = pd.date_range(datetime.today(), periods=len(daily_summary_list)).tolist()\n",
        "datelist = [date.date().strftime('%Y-%m-%d') for date in datelist]\n",
        "\n",
        "\n",
        "# Map dates to descriptions\n",
        "weather_data = {date: desc for date, desc in zip(datelist, daily_summary_list)}\n",
        "\n",
        "# Convert to JSON\n",
        "weather_json = json.dumps(weather_data, indent=4)\n",
        "print(weather_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUv9TYlTagrn",
        "outputId": "bc7b8532-1f89-4146-8dc0-b47c6826c02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"2025-03-23\": \"Partly cloudy and light winds\",\n",
            "    \"2025-03-24\": \"Sunny intervals and a gentle breeze\",\n",
            "    \"2025-03-25\": \"Light rain showers and light winds\",\n",
            "    \"2025-03-26\": \"Sunny intervals and a gentle breeze\",\n",
            "    \"2025-03-27\": \"Sunny intervals and a moderate breeze\",\n",
            "    \"2025-03-28\": \"Drizzle and a moderate breeze\",\n",
            "    \"2025-03-29\": \"Sunny intervals and a gentle breeze\",\n",
            "    \"2025-03-30\": \"Sunny intervals and a gentle breeze\",\n",
            "    \"2025-03-31\": \"Sunny intervals and a gentle breeze\",\n",
            "    \"2025-04-01\": \"Light rain showers and a gentle breeze\",\n",
            "    \"2025-04-02\": \"Sunny intervals and a moderate breeze\",\n",
            "    \"2025-04-03\": \"Drizzle and a gentle breeze\",\n",
            "    \"2025-04-04\": \"Light rain and a gentle breeze\",\n",
            "    \"2025-04-05\": \"Heavy rain showers and a gentle breeze\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5\n",
        "1. run below code and change city as per ur requirement."
      ],
      "metadata": {
        "id": "Tyt7rFPMD0XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the required library\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# Activate the Nominatim geocoder\n",
        "locator = Nominatim(user_agent=\"myGeocoder\")\n",
        "\n",
        "# Geocode the city Mexico City in Mexico\n",
        "location = locator.geocode(\"Mexico City, Mexico\")\n",
        "\n",
        "# Check if the location was found\n",
        "if location:\n",
        "    # Retrieve the bounding box\n",
        "    bounding_box = location.raw.get('boundingbox', [])\n",
        "\n",
        "    # Check if the bounding box is available\n",
        "    if len(bounding_box) > 1:\n",
        "        # Extract the minimum latitude from the bounding box (the first value in the list)\n",
        "        min_latitude = bounding_box[0]\n",
        "        print(f\"The minimum latitude of the bounding box for Mexico City is: {min_latitude}\")\n",
        "    else:\n",
        "        print(\"Bounding box information not available.\")\n",
        "else:\n",
        "    print(\"Location not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cibl_tMpGFyj",
        "outputId": "8e4c4b5f-01ce-4ea6-986c-937b3fc99513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum latitude of the bounding box for Mexico City is: 19.0487187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6\n",
        "1. run below code and get link\n",
        "2. change required values based on your question"
      ],
      "metadata": {
        "id": "ZxaPhOodGmPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "\n",
        "# Fetch the feed with posts mentioning \"gpt\" and having at least 37 points\n",
        "feed_url = \"https://hnrss.org/newest?q=gpt&points=37\"\n",
        "feed = feedparser.parse(feed_url)\n",
        "\n",
        "# Extract the link of the latest post\n",
        "if feed.entries:\n",
        "    print(feed.entries[0].link)  # Print the link of the latest post"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DGoRGtqGmpO",
        "outputId": "0c0e4949-47fe-4ceb-a316-b058b22d5c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://twitter.com/Baidu_Inc/status/1901089355890036897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import json\n",
        "\n",
        "def fetch_latest_gpt_post(query=\"gpt\", points=37):\n",
        "    # Fetch the feed with posts based on query and minimum points\n",
        "    feed_url = f\"https://hnrss.org/newest?q={query}&points={points}\"\n",
        "    feed = feedparser.parse(feed_url)\n",
        "\n",
        "    # Extract the link of the latest post\n",
        "    if feed.entries:\n",
        "        latest_post_link = feed.entries[0].link\n",
        "    else:\n",
        "        latest_post_link = \"No posts found.\"\n",
        "\n",
        "    # Return the result as JSON schema\n",
        "    return json.dumps({\"answer\": latest_post_link})\n",
        "\n",
        "# Example usage\n",
        "print(fetch_latest_gpt_post(\"gpt\", 37))"
      ],
      "metadata": {
        "id": "VsWUWRKLjrYw",
        "outputId": "ad204136-dd5f-4bd9-c4ed-cff393dec031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"answer\": \"https://twitter.com/Baidu_Inc/status/1901089355890036897\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7\n",
        "1. change github token with yours\n",
        "2. change city and followers with your value"
      ],
      "metadata": {
        "id": "CyUmoi6ZOaxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final my code\n",
        "import requests\n",
        "import datetime\n",
        "\n",
        "# Your GitHub personal access token (Replace with a valid token)\n",
        "GITHUB_TOKEN = 'Your_token'\n",
        "headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n",
        "\n",
        "# Get the current local date and time\n",
        "current_local_datetime = datetime.datetime.now()\n",
        "\n",
        "# Subtract 5 minutes to set the cutoff time\n",
        "cutoff_datetime = current_local_datetime - datetime.timedelta(minutes=5)\n",
        "\n",
        "# Function to search for users based on location and followers, sorted by join date\n",
        "def search_users():\n",
        "    url = \"https://api.github.com/search/users?q=location:Paris+followers:>190&sort=joined&order=desc\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get('items', [])\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code} - {response.json().get('message')}\")\n",
        "        return []\n",
        "\n",
        "# Get users in Paris with more than 190 followers, sorted by join date (newest first)\n",
        "users = search_users()\n",
        "\n",
        "# Process the first valid user who is not ultra-new\n",
        "for user in users:\n",
        "    user_url = user['url']  # Get the profile API URL\n",
        "    user_response = requests.get(user_url, headers=headers)\n",
        "\n",
        "    if user_response.status_code == 200:\n",
        "        user_data = user_response.json()\n",
        "        created_at = user_data['created_at']  # ISO 8601 format\n",
        "        created_at_date = datetime.datetime.fromisoformat(created_at[:-1])  # Convert to datetime\n",
        "\n",
        "        # Check if the user is NOT ultra-new (joined more than 5 minutes ago)\n",
        "        if created_at_date < cutoff_datetime:\n",
        "            print(f\"First valid user joined at: {created_at}\")\n",
        "            print(f\"User details: {user_data['login']} ({user_data['html_url']})\")\n",
        "            break  # Stop after finding the first valid user\n",
        "    else:\n",
        "        print(f\"Error fetching user details: {user_response.status_code}\")\n",
        "\n",
        "# If no valid users found\n",
        "else:\n",
        "    print(\"No valid users found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFFu9A7hVRAe",
        "outputId": "2aebf165-563f-4bba-809e-64e9e6585d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First valid user joined at: 2023-06-19T09:01:12Z\n",
            "User details: jbilcke-hf (https://github.com/jbilcke-hf)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8\n",
        "1. create new github repo.\n",
        "2. Create the directory .github/workflows/ if it doesn’t already exist.\n",
        "3. Inside that directory, create a file named daily-commit.yml.\n",
        "4. Add the following below content to daily-commit.yml.\n",
        "5. Commit and push daily-commit.yml to your repository.\n",
        "6. Go to the Actions tab in your GitHub repository.\n",
        "7. Manually trigger the workflow using the \"Run workflow\" button under \"Daily Commit\".\n",
        "8. Wait for the workflow to complete.\n",
        "9. Verify that a new commit appears in your repository history.\n",
        "10. Enter your repository URL (format: https://github.com/USER/REPO): https://github.com/YOUR_USERNAME/YOUR_REPO\n",
        "\n",
        "11. add your student mail"
      ],
      "metadata": {
        "id": "QET2oIhqhG-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name: Daily Commit\n",
        "\n",
        "on:\n",
        "  schedule:\n",
        "    - cron: '30 2 * * *'  # Runs daily at 02:30 UTC\n",
        "  workflow_dispatch:  # Allows manual triggering\n",
        "\n",
        "permissions:\n",
        "  contents: write  # Ensure GitHub Actions can push changes\n",
        "\n",
        "jobs:\n",
        "  commit:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - name: Checkout Repository\n",
        "        uses: actions/checkout@v4\n",
        "\n",
        "      - name: Configure Git (21********@ds.study.iitm.ac.in)\n",
        "        run: |\n",
        "          git config --global user.name \"GitHub Action\"\n",
        "          git config --global user.email \"21******@ds.study.iitm.ac.in\"\n",
        "\n",
        "      - name: Make a Change\n",
        "        run: |\n",
        "          echo \"Last run: $(date)\" > last_run.txt  # Always modify the file\n",
        "\n",
        "      - name: Commit and Push Changes\n",
        "        run: |\n",
        "          git add last_run.txt\n",
        "          git commit -m \"Automated daily commit at $(date)\" || echo \"No changes to commit\"\n",
        "          git push\n"
      ],
      "metadata": {
        "id": "66MgRbr5j_xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9\n",
        "1. i did manually, by converting pdf to excel initially\n",
        "2. You can try below code refered from TDS group\n",
        "3. Filter condition should be modify based on your question\n",
        "4. Working for me checked"
      ],
      "metadata": {
        "id": "ConjFi_4hjkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula-py\n",
        "import tabula\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Path to the PDF file\n",
        "pdf_path = pdf_path = list(files.upload().keys())[0]\n",
        "\n",
        "# Extract tables from the PDF, specifying pages and multiple_tables=True\n",
        "tables = tabula.read_pdf(pdf_path, pages=\"all\", multiple_tables=True)\n",
        "\n",
        "# Initialize an empty list to store all DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Iterate through each table and add a \"Group\" column based on the page number\n",
        "for i, table in enumerate(tables):\n",
        "    # Add a \"Group\" column to the table\n",
        "    table[\"Group\"] = i + 1  # Group 1 for Page 1, Group 2 for Page 2, etc.\n",
        "    # Append the table to the list\n",
        "    all_dfs.append(table)\n",
        "\n",
        "# Combine all DataFrames into a single DataFrame\n",
        "df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# Rename columns for easier access (if necessary)\n",
        "df.columns = [\"Maths\", \"Physics\", \"English\", \"Economics\", \"Biology\", \"Group\"]\n",
        "\n",
        "# Convert marks to numerical data types\n",
        "df[\"Maths\"] = pd.to_numeric(df[\"Maths\"], errors=\"coerce\")\n",
        "df[\"Physics\"] = pd.to_numeric(df[\"Physics\"], errors=\"coerce\")\n",
        "df[\"English\"] = pd.to_numeric(df[\"English\"], errors=\"coerce\")\n",
        "df[\"Economics\"] = pd.to_numeric(df[\"Economics\"], errors=\"coerce\")\n",
        "df[\"Biology\"] = pd.to_numeric(df[\"Biology\"], errors=\"coerce\")\n",
        "df[\"Group\"] = pd.to_numeric(df[\"Group\"], errors=\"coerce\")\n",
        "\n",
        "# Drop rows with missing values (if any)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display the first few rows of the combined DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Display the data types of the columns\n",
        "print(df.dtypes)\n",
        "filtered_df = df[(df[\"Physics\"] >= 56) & (df[\"Group\"].between(46, 79))]\n",
        "\n",
        "total_biology_marks = filtered_df[\"Economics\"].sum()\n",
        "print(total_biology_marks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "lQvivqlYpb_m",
        "outputId": "881585ea-d5c5-4f2b-a0d8-5d0794634fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabula-py in /usr/local/lib/python3.11/dist-packages (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.11/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.11/dist-packages (from tabula-py) (1.26.4)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.11/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3fb3a5d-40b9-4df6-906e-f0659cc944d0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3fb3a5d-40b9-4df6-906e-f0659cc944d0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving q-extract-tables-from-pdf.pdf to q-extract-tables-from-pdf (1).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Feb 09, 2025 12:27:26 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Feb 09, 2025 12:27:26 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Maths  Physics  English  Economics  Biology  Group\n",
            "0     90       27       14         52       45      1\n",
            "1     88       34       35         48       79      1\n",
            "2     14       11       77         38       42      1\n",
            "3     41       84       71         45       58      1\n",
            "4     70       19       67         82       84      1\n",
            "Maths        int64\n",
            "Physics      int64\n",
            "English      int64\n",
            "Economics    int64\n",
            "Biology      int64\n",
            "Group        int64\n",
            "dtype: object\n",
            "26272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10\n",
        "credit Shaon Ballav\n",
        "\n",
        "Refer: https://chatgpt.com/share/67a5ea5d-07fc-8007-8512-ce17cd7d969a\n",
        "\n",
        "First go to sources in inspect element and find exam-tds-2025-01-ga4.js file\n",
        "\n",
        "\n",
        "Go to line 776, find the function which starts like\n",
        "\n",
        " async function xt({user.....\n",
        "\n",
        "You have to add breakpoint here:\n",
        "\n",
        "Locate the let s = async c => { function\n",
        "\n",
        "\n",
        "Set a breakpoint inside the function:\n",
        "\n",
        "\n",
        "console.log(\"Expected Answer (r):\", r);\n",
        "console.log(\"Your Answer (c):\", c);\n",
        "\n",
        "This will print their values when the function executes.\n"
      ],
      "metadata": {
        "id": "fxURojaBhj-d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYpxoXKpbmh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}