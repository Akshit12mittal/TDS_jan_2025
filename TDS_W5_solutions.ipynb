{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Hi ppl, use this notebook to verify/solve your answers.\n",
        "- You can connect with me on LinkedIn: https://www.linkedin.com/in/akshit12mittal\n",
        "- PS- I didn't solve any ques, I hacked ga5 but in case u want to understand things, here's the colab."
      ],
      "metadata": {
        "id": "p81XtBphZFd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1\n",
        "1. Run the code."
      ],
      "metadata": {
        "id": "_duq8RKGZVyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpMWkxqKYuFt",
        "outputId": "71b46bf5-bf55-4bbc-b9b2-3b2999fda24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (1000, 7)\n",
            "Unique countries after mapping: ['BR', 'Bra', 'Brazil', 'FRANCE', 'Fra', 'France', 'GB', 'INDIA', 'Ind', 'India', 'UAE', 'US', 'United Arab Emirates', 'United Kingdom', 'United States']\n",
            "Unique products after extraction: ['Alpha', 'Beta', 'Delta', 'Epsilon', 'Eta', 'Gamma', 'Iota', 'Kappa', 'Theta', 'Zeta']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload Excel File\n",
        "uploaded = files.upload()  # Manually upload the file\n",
        "file_name = list(uploaded.keys())[0]  # Get the uploaded filename\n",
        "\n",
        "# Step 2: Load Data\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "# Step 3: Trim and Normalize Country Names\n",
        "df['Country'] = df['Country'].str.strip().str.upper()  # Remove spaces and convert to uppercase\n",
        "\n",
        "# Step 4: Standardize Country Names\n",
        "country_mapping = {\n",
        "    \"USA\": \"US\", \"U.S.A\": \"US\", \"UNITED STATES\": \"US\", \"US\": \"US\",\n",
        "    \"BRA\": \"BR\", \"BRAZIL\": \"BR\", \"BR\": \"BR\",\n",
        "    \"U.K\": \"GB\", \"UK\": \"GB\", \"UNITED KINGDOM\": \"GB\",\n",
        "    \"FR\": \"FRANCE\", \"FRA\": \"FRANCE\", \"FRANCE\": \"FRANCE\",\n",
        "    \"IND\": \"INDIA\", \"IN\": \"INDIA\", \"INDIA\": \"INDIA\",\n",
        "    \"AE\": \"UAE\", \"U.A.E\": \"UAE\", \"UNITED ARAB EMIRATES\": \"UAE\", \"UAE\": \"UAE\"\n",
        "}\n",
        "df['Country'] = df['Country'].replace(country_mapping)\n",
        "\n",
        "# Step 5: Standardize Date Formats\n",
        "def convert_to_datetime(date):\n",
        "    for fmt in (\"%m-%d-%Y\", \"%Y/%m/%d\", \"%Y-%m-%d\"):\n",
        "        try:\n",
        "            return datetime.strptime(str(date), fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT  # Return NaT for unrecognized formats\n",
        "\n",
        "df['Date'] = df['Date'].apply(convert_to_datetime)\n",
        "\n",
        "# Step 6: Extract Product Name (Before \"/\")\n",
        "df['Product/Code'] = df['Product/Code'].str.split('/').str[0].str.strip()\n",
        "\n",
        "# Step 7: Clean and Convert Sales & Cost Columns\n",
        "df['Sales'] = df['Sales'].astype(str).str.replace(\"USD\", \"\").str.strip().astype(float)\n",
        "df['Cost'] = df['Cost'].astype(str).str.replace(\"USD\", \"\").str.strip()\n",
        "\n",
        "# Handle missing cost values (assume 50% of Sales)\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Cost'].fillna(df['Sales'] * 0.5, inplace=True)\n",
        "\n",
        "# Step 8: Apply Filters\n",
        "date_filter = datetime(2023, 1, 3, 16, 56, 57)  # Tue Jan 03 2023 16:56:57 GMT+0530\n",
        "df_filtered = df[\n",
        "    (df['Date'] <= date_filter) &\n",
        "    (df['Product/Code'].str.lower() == \"eta\") &\n",
        "    (df['Country'] == \"BR\")\n",
        "]\n",
        "\n",
        "# Step 9: Calculate Total Margin\n",
        "total_sales = df_filtered['Sales'].sum()\n",
        "total_cost = df_filtered['Cost'].sum()\n",
        "\n",
        "if total_sales > 0:\n",
        "    total_margin = (total_sales - total_cost) / total_sales\n",
        "else:\n",
        "    total_margin = 0\n",
        "\n",
        "# Step 10: Display Results\n",
        "print(f\"Total Sales: {total_sales:.2f}\")\n",
        "print(f\"Total Cost: {total_cost:.2f}\")\n",
        "print(f\"Total Margin: {total_margin:.4f}\")\n",
        "\n",
        "# Step 11: Save Cleaned Data (Optional)\n",
        "#df_filtered.to_csv(\"cleaned_data.csv\", index=False)\n",
        "#files.download(\"cleaned_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2\n",
        "1.  Run the code."
      ],
      "metadata": {
        "id": "mEGeNNAq92mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# Upload the text file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Data Extraction: Read file line by line and extract student IDs\n",
        "student_ids = set()\n",
        "with open(file_name, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        matches = re.findall(r'\\b[A-Z0-9]{10}\\b', line)  # Match exact 10-character alphanumeric IDs\n",
        "        student_ids.update(matches)\n",
        "\n",
        "# Reporting: Count the number of unique student IDs\n",
        "print(f\"Number of unique students: {len(student_ids)}\")"
      ],
      "metadata": {
        "id": "xj9WSkSl960y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3\n",
        "1.  Questions might be different, change things accordingly."
      ],
      "metadata": {
        "id": "Glapv6xF-BDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import re\n",
        "import os\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from google.colab import files\n",
        "\n",
        "# Function to compute SHA-256 hash\n",
        "def compute_hash(text):\n",
        "    return hashlib.sha256(text.encode()).hexdigest()\n",
        "\n",
        "# Function to parse Apache log entries\n",
        "def parse_log_line(line):\n",
        "    log_pattern = (r'^(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(\\S+) (.*?) (\\S+)\" (\\d+) (\\S+) \"(.*?)\" \"(.*?)\" (\\S+) (\\S+)$')\n",
        "    match = re.match(log_pattern, line)\n",
        "    if match:\n",
        "        return {\n",
        "            \"ip\": match.group(1),\n",
        "            \"time\": match.group(4),\n",
        "            \"method\": match.group(5),\n",
        "            \"url\": match.group(6),\n",
        "            \"protocol\": match.group(7),\n",
        "            \"status\": int(match.group(8)),\n",
        "            \"size\": match.group(9),\n",
        "            \"referer\": match.group(10),\n",
        "            \"user_agent\": match.group(11),\n",
        "            \"vhost\": match.group(12),\n",
        "            \"server\": match.group(13)\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # Get uploaded file name\n",
        "\n",
        "# Load and parse the log file\n",
        "def load_logs(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    parsed_logs = []\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parsed_entry = parse_log_line(line)\n",
        "            if parsed_entry:\n",
        "                parsed_logs.append(parsed_entry)\n",
        "    return pd.DataFrame(parsed_logs)\n",
        "\n",
        "# Convert time format\n",
        "def convert_time(timestamp):\n",
        "    return datetime.strptime(timestamp, \"%d/%b/%Y:%H:%M:%S %z\")\n",
        "\n",
        "df = load_logs(file_path)\n",
        "\n",
        "if not df.empty:\n",
        "    df[\"datetime\"] = df[\"time\"].apply(convert_time)\n",
        "    df[\"day_of_week\"] = df[\"datetime\"].dt.strftime('%A')\n",
        "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "\n",
        "    # Filter conditions\n",
        "    filtered_df = df[\n",
        "        (df[\"method\"] == \"GET\") &\n",
        "        (df[\"url\"].str.startswith(\"/telugu/\")) &\n",
        "        (df[\"status\"] >= 200) & (df[\"status\"] < 300) &\n",
        "        (df[\"day_of_week\"] == \"Sunday\") &\n",
        "        (df[\"hour\"] >= 5) &\n",
        "        (df[\"hour\"] < 10)\n",
        "    ]\n",
        "\n",
        "    # Compute hash of the result\n",
        "    result_hash = compute_hash(str(len(filtered_df)))\n",
        "\n",
        "    # Output the count and hash\n",
        "    print(\"Successful GET requests for /telugu/ on Sundays (5:00-9:59 AM):\", len(filtered_df))\n",
        "else:\n",
        "    print(\"No log data available for processing.\")"
      ],
      "metadata": {
        "id": "YQUCoY-z-E2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4\n",
        "1.  Questions might be different, change things accordingly."
      ],
      "metadata": {
        "id": "7mNlmAGq-NwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import re\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # Get uploaded file name\n",
        "\n",
        "# Function to parse Apache log entries\n",
        "def parse_log_line(line):\n",
        "    log_pattern = (r'^(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(\\S+) (.*?) (\\S+)\" (\\d+) (\\S+) \"(.*?)\" \"(.*?)\" (\\S+) (\\S+)$')\n",
        "    match = re.match(log_pattern, line)\n",
        "    if match:\n",
        "        return {\n",
        "            \"ip\": match.group(1),\n",
        "            \"time\": match.group(4),\n",
        "            \"method\": match.group(5),\n",
        "            \"url\": match.group(6),\n",
        "            \"protocol\": match.group(7),\n",
        "            \"status\": int(match.group(8)),\n",
        "            \"size\": int(match.group(9)) if match.group(9).isdigit() else 0,\n",
        "            \"referer\": match.group(10),\n",
        "            \"user_agent\": match.group(11),\n",
        "            \"vhost\": match.group(12),\n",
        "            \"server\": match.group(13)\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# Load and parse the log file\n",
        "def load_logs(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    parsed_logs = []\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parsed_entry = parse_log_line(line)\n",
        "            if parsed_entry:\n",
        "                parsed_logs.append(parsed_entry)\n",
        "    return pd.DataFrame(parsed_logs)\n",
        "\n",
        "# Convert time format\n",
        "def convert_time(timestamp):\n",
        "    return datetime.strptime(timestamp, \"%d/%b/%Y:%H:%M:%S %z\")\n",
        "\n",
        "df = load_logs(file_path)\n",
        "\n",
        "if not df.empty:\n",
        "    df[\"datetime\"] = df[\"time\"].apply(convert_time)\n",
        "    df[\"date\"] = df[\"datetime\"].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Filter conditions for /hindimp3/ on 2024-05-16\n",
        "    filtered_df = df[\n",
        "        (df[\"url\"].str.startswith(\"/hindimp3/\")) &\n",
        "        (df[\"date\"] == \"2024-05-16\")\n",
        "    ]\n",
        "\n",
        "    # Aggregate data by IP\n",
        "    ip_data = filtered_df.groupby(\"ip\")[\"size\"].sum().reset_index()\n",
        "\n",
        "    # Identify the top data consumer\n",
        "    top_ip = ip_data.loc[ip_data[\"size\"].idxmax()]\n",
        "\n",
        "    # Output the result\n",
        "    print(f\"Top IP Address: {top_ip['ip']}\")\n",
        "    print(f\"Total Bytes Downloaded: {top_ip['size']}\")\n",
        "else:\n",
        "    print(\"No log data available for processing.\")"
      ],
      "metadata": {
        "id": "wOEGpBzm-Sie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5\n",
        "1. run below code and change city as per ur requirement in known_cities.\n",
        "2. change product and minimum qty."
      ],
      "metadata": {
        "id": "1UtTAStq-cyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy python-Levenshtein\n",
        "\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz, process\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the JSON file\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "df = pd.read_json(file_path)\n",
        "\n",
        "# List of known city names (add more as needed)\n",
        "known_cities = [\"Jakarta\"]\n",
        "\n",
        "def cluster_cities(city_series):\n",
        "    \"\"\"Standardize city names using fuzzy matching.\"\"\"\n",
        "    city_series = city_series.fillna('Unknown')\n",
        "\n",
        "    def get_closest_city(city_name):\n",
        "        best_match = process.extractOne(city_name, known_cities, scorer=fuzz.token_set_ratio)\n",
        "        if best_match and best_match[1] > 90:  # Adjust the threshold as needed\n",
        "            return best_match[0]\n",
        "        return city_name\n",
        "\n",
        "    city_series = city_series.apply(get_closest_city)\n",
        "    return city_series\n",
        "\n",
        "# Standardize city names\n",
        "df['city'] = cluster_cities(df['city'])\n",
        "\n",
        "# Debug: Check unique city names after clustering\n",
        "print(\"Unique cities after clustering:\", df['city'].unique())\n",
        "\n",
        "# Filter sales for Shoes with at least 32 units\n",
        "df_filtered = df[(df['product'] == \"Shoes\") & (df['sales'] >= 32)]\n",
        "\n",
        "# Aggregate sales by city\n",
        "df_grouped = df_filtered.groupby(\"city\")[\"sales\"].sum().reset_index()\n",
        "\n",
        "# Identify the top-performing city\n",
        "top_city = df_grouped.sort_values(by=\"sales\", ascending=False).iloc[0]\n",
        "\n",
        "# Find sales for Jakarta\n",
        "jakarta_sales = df_grouped[df_grouped[\"city\"].str.lower() == \"jakarta\"][\"sales\"].sum()\n",
        "\n",
        "print(f\"Total Shoes units sold in Jakarta (with at least 32 units per transaction): {jakarta_sales}\")"
      ],
      "metadata": {
        "id": "45uuVfMU-c9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6\n",
        "1."
      ],
      "metadata": {
        "id": "hAadgI8A-lU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7\n",
        "1."
      ],
      "metadata": {
        "id": "2vgWO8Q--4FV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8\n",
        "1.  Questions might be different, change things accordingly."
      ],
      "metadata": {
        "id": "PSa0hleY-51e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT post_id\n",
        "FROM (\n",
        "    SELECT post_id\n",
        "    FROM (\n",
        "        SELECT post_id,\n",
        "               json_extract(comments, '$[*].stars.useful') AS useful_stars\n",
        "        FROM social_media\n",
        "        WHERE timestamp >= '2025-01-10T01:12:43.572Z'\n",
        "    )\n",
        "    WHERE EXISTS (\n",
        "        SELECT 1 FROM UNNEST(useful_stars) AS t(value)\n",
        "        WHERE CAST(value AS INTEGER) >= 4\n",
        "    )\n",
        ")\n",
        "ORDER BY post_id;"
      ],
      "metadata": {
        "id": "aaTrAL-G-8fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9\n",
        "1. download your video and upload in colab file as videoplaback.mp4\n",
        "2. change your times as per question in code\n",
        "3. text received from here need improvement use chatgpt and tell chatgpt to add full stop and comma in given text."
      ],
      "metadata": {
        "id": "ichOyQYE_Lj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "import io\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def extract_audio_from_video(video_path, start_sec, end_sec, output_audio_path=\"temp_audio.wav\"):\n",
        "    \"\"\"Extracts audio from a video segment and saves it as a WAV file.\"\"\"\n",
        "    with VideoFileClip(video_path) as video:\n",
        "      audio_clip = video.audio.subclip(start_sec, end_sec)\n",
        "      audio_clip.write_audiofile(output_audio_path)\n",
        "\n",
        "def transcribe_audio(audio_filename=\"temp_audio.wav\",  output_txt_file=\"transcription.txt\"):\n",
        "    \"\"\"Transcribes the audio file using Google Web Speech API.\"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_filename) as source:\n",
        "        audio = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        transcript = recognizer.recognize_google(audio)\n",
        "        print(\"Transcript:\")\n",
        "        print(transcript)\n",
        "\n",
        "        # Save transcript to a text file\n",
        "        with open(output_txt_file, \"w\") as f:\n",
        "            f.write(transcript)\n",
        "        print(f\"Transcript saved to {output_txt_file}\")\n",
        "\n",
        "        return transcript\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, I could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
        "\n",
        "# --- Usage ---\n",
        "\n",
        "# Upload the video file\n",
        "# uploaded = files.upload()\n",
        "# video_file = list(uploaded.keys())[0]\n",
        "video_file = \"/content/videoplayback.mp4\"\n",
        "\n",
        "# Extract audio from the video segment (adjust start and end times as needed)\n",
        "extract_audio_from_video(video_file, 407.5, 519.6)\n",
        "\n",
        "# Transcribe the extracted audio\n",
        "transcribe_audio()"
      ],
      "metadata": {
        "id": "cwGBZ9jF_LzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10\n"
      ],
      "metadata": {
        "id": "d8pel6S5_RdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load the scrambled image\n",
        "scrambled_image = Image.open('jigsaw.webp')  # Replace with your file path if needed\n",
        "\n",
        "# Define the mapping data\n",
        "mapping_data = [\n",
        "    (2, 1, 0, 0), (1, 1, 0, 1), (4, 1, 0, 2), (0, 3, 0, 3), (0, 1, 0, 4),\n",
        "    (1, 4, 1, 0), (2, 0, 1, 1), (2, 4, 1, 2), (4, 2, 1, 3), (2, 2, 1, 4),\n",
        "    (0, 0, 2, 0), (3, 2, 2, 1), (4, 3, 2, 2), (3, 0, 2, 3), (3, 4, 2, 4),\n",
        "    (1, 0, 3, 0), (2, 3, 3, 1), (3, 3, 3, 2), (4, 4, 3, 3), (0, 2, 3, 4),\n",
        "    (3, 1, 4, 0), (1, 2, 4, 1), (1, 3, 4, 2), (0, 4, 4, 3), (4, 0, 4, 4)\n",
        "]\n",
        "\n",
        "# Create a blank image for the reconstructed result\n",
        "reconstructed_image = Image.new('RGB', (500, 500))\n",
        "\n",
        "# Loop through each mapping and place pieces in their original positions\n",
        "piece_size = scrambled_image.width // 5  # Each piece is assumed to be square\n",
        "for original_row, original_col, scrambled_row, scrambled_col in mapping_data:\n",
        "    # Calculate coordinates of the scrambled piece\n",
        "    left = scrambled_col * piece_size\n",
        "    upper = scrambled_row * piece_size\n",
        "    right = left + piece_size\n",
        "    lower = upper + piece_size\n",
        "\n",
        "    # Crop the piece from the scrambled image\n",
        "    piece = scrambled_image.crop((left, upper, right, lower))\n",
        "\n",
        "    # Calculate coordinates for placing the piece in the reconstructed image\n",
        "    dest_left = original_col * piece_size\n",
        "    dest_upper = original_row * piece_size\n",
        "\n",
        "    # Paste the piece into its correct position\n",
        "    reconstructed_image.paste(piece, (dest_left, dest_upper))\n",
        "\n",
        "# Save the reconstructed image as PNG\n",
        "reconstructed_image.save('reconstructed_image.png')\n",
        "print(\"Reconstructed image saved as 'reconstructed_image.png'\")"
      ],
      "metadata": {
        "id": "DCHVbkK4_YBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}